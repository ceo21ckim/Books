{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 3219\n",
      "#Selected categories: ['alt.atheism', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.crypt', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', 'comp.sys.ibm.pc.hardware', 'sci.crypt']\n",
    "\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "print('#Train set size:', len(newsgroups_train.data))\n",
    "print('#Selected categories:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "cv = CountVectorizer(token_pattern=\"[\\w']{3,}\", stop_words='english', max_features=2000, min_df=5, max_df=0.5)\n",
    "review_cv = cv.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#shape of review_topics: (3219, 10)\n",
      "#Sample of review_topics: [0.903 0.007 0.027 0.008 0.007 0.008 0.007 0.007 0.007 0.018]\n",
      "#Sum of topic weights of documents: [0.087 0.083 0.085 0.115 0.115 0.126 0.098 0.072 0.07  0.148]\n",
      "#shape of topic word distribution: (10, 2000)\n"
     ]
    }
   ],
   "source": [
    "# LDA -> LatentDirichletAllocation \n",
    "# hyper-parameter : max_iter, learning_method, n_jobs, random_state\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, # 추출할 topic의 수 \n",
    "                                max_iter=5, \n",
    "                                topic_word_prior=0.1, doc_topic_prior=1.0, \n",
    "                                learning_method='online', \n",
    "                                n_jobs= -1, # 사용 processor 수\n",
    "                                random_state=0)\n",
    "\n",
    "review_topics = lda.fit_transform(review_cv)\n",
    "\n",
    "print('#shape of review_topics:', review_topics.shape)\n",
    "print('#Sample of review_topics:', review_topics[0])\n",
    "\n",
    "gross_topic_weights = np.mean(review_topics, axis=0)\n",
    "print('#Sum of topic weights of documents:', gross_topic_weights)\n",
    "print('#shape of topic word distribution:', lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96cf18f209edf6220e7043c3825950920f4c7ad96ff42ffae85e8b73f5a9541f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
